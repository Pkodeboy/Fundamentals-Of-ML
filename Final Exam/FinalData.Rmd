---
title: "Final Exam"
author: "Pranay Kumar Kodeboyina"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
  word_document: default
  pdf_document: default
---

```{r}
# importing the Necessary libraries
library(dplyr)
library(caret)
library(factoextra)
library(leaps)
library(dbscan)
library(esquisse)
library(readr)
```

```{r}
#Extracting the current working directory
getwd()

#Loading PUDL csv data Import the data set into R
Fuel <- read.csv("data/fuel_receipts_costs.csv", header = TRUE, sep = ",", stringsAsFactors = TRUE)

#Observing the first 10 observations of the data set
head(Fuel, n=10L)

#Observing the data sets that have missing values more than 50 percent and omitting those columns in the analysis 

# calculate the percentage of missing values in each column
missing_pct <- colMeans(is.na(Fuel)) * 100

# create a data frame with the missing percentages as a single column
missing_df <- data.frame(percent_missing = missing_pct)

# print the resulting data frame
print(missing_df)

#Upon observing the data we can see that the following columns has missing values more than 50% - mine_id_pudl, moisture_content_pct, chlorine_content_ppm 

```

```{r}
# set the random seed for reproducibility
set.seed(8627)

# create a new data set without the specified columns and considering the columns without the specified columns
Fuel_data_ALL <- Fuel[, -c(1,2,3,5,9,10,15,18,19,20,21,22,23)]
head(Fuel_data_ALL)

#head(Fuel_data_ALL)

# calculate the number of rows to select (2% of the total rows)
nrows <- round(nrow(Fuel_data_ALL) * 0.02)

# randomly select the rows
idx <- sample(nrow(Fuel_data_ALL), nrows, replace = FALSE)

# create the new data set with the randomly selected rows
Fuel_data <- Fuel_data_ALL[idx, ]

#View(Fuel_data)

#Exploratory data Analysis
# Create histograms of sulphur and ash content
ggplot(Fuel_data, aes(x=sulfur_content_pct)) +
  geom_histogram(binwidth=0.5, color="black", fill="lightblue") +
  labs(x="Sulphur Content (%)", y="Frequency", title="Histogram of Sulphur Content")

ggplot(Fuel_data, aes(x=ash_content_pct)) +
  geom_histogram(binwidth=0.5, color="black", fill="lightgreen") +
  labs(x="Ash Content (%)", y="Frequency", title="Histogram of Ash Content")

# create a bar graph of the fuel_type_code_pudl categories
ggplot(Fuel_data, aes(x = fuel_type_code_pudl)) +
  geom_bar(fill = "blue") +
  labs(title = "Fuel Type Distribution",
       x = "Fuel Type Code PUDL",
       y = "Count")


# Create dummy variables for fuel_type_code_pudl to create categorical in to numeric variables
Fuel_data <- Fuel_data %>% 
  mutate(fuel_type_gas = ifelse(fuel_type_code_pudl == "gas", 1, 0),
         fuel_type_coal = ifelse(fuel_type_code_pudl == "coal", 1, 0),
         fuel_type_oil = ifelse(fuel_type_code_pudl == "oil", 1, 0))

# Remove the original fuel_type_code_pudl column
Fuel_data <- select(Fuel_data, -fuel_type_code_pudl)


#We can finding the missing values in fuel_cost_per_mmbtu data and replacing them with the median values for the analysis 

# Calculate the median value of fuel_cost_per_mmbtu
median_value <- median(Fuel_data$fuel_cost_per_mmbtu, na.rm = TRUE)

# Replace missing values with median
Fuel_data$fuel_cost_per_mmbtu[is.na(Fuel_data$fuel_cost_per_mmbtu)] <- median_value

```

```{r}

# Generate a vector of random indices for the data frame and splitiing 75% of data 
train_idx <- sample(nrow(Fuel_data), round(nrow(Fuel_data) * 0.75), replace = FALSE)

# Create the training set
train_data <- Fuel_data[train_idx, ]

# Create the test set by excluding the training set indices
test_data <- Fuel_data[-train_idx, ]

nrow(train_data)

nrow(test_data)

View(train_data)


library(caret)

# Create a preprocessing object using the train_data dataset
preproc_obj <- preProcess(train_data[, 4:7], method = c("center", "scale"))

# Use the preprocessing object to normalize the train_data and test_data datasets
train_data_norm <- predict(preproc_obj, train_data)
test_data_norm <- predict(preproc_obj, test_data)



#View(train_data_norm)

```

```{r}

#We have divided the data in to Train and Test set and we are finding Optimal values of K using Gap stat method and Silhouette method

#In order determine the optimal value of k, employing different methods to determine k
library(cluster)
library(ggplot2)

wss <- c()
for (i in 1:10) {
  kmeans_fit <- kmeans(train_data_norm[, 4:7], i, nstart = 25)
  wss[i] <- kmeans_fit$tot.withinss
}

elbow_df <- data.frame(K = 1:10, WSS = wss)

ggplot(elbow_df, aes(x = K, y = WSS)) + 
  geom_line() + 
  geom_point() +
  labs(x = "Number of Clusters (K)", y = "Within-Cluster Sum of Squares (WSS)") +
  ggtitle("Elbow Method with Slope") +
  geom_smooth(method = "lm", se = FALSE)


fviz_nbclust(train_data_norm[, 4:7], kmeans, method = "silhouette") + labs(subtitle = "Silhouette Method To Determine Optimal Value of K")

#From the above method we can see that using Elbow method the value of K is 5 and the value of K is 5 with the silhouette method. I am choosing K value of 5 to cluster the data as it has produced clusters with clear gap between each other. 
```

```{r}

kmean <- kmeans(train_data_norm[, 4:7], centers = 6)

## Below finding cluster center for all rows and colomns
kmean$centers

## Number of observation in each cluster
kmean$size

library(factoextra)
##For the above observations applyng the K means clustering to visualize the 5 clusters
fviz_cluster(kmean, data=train_data_norm[,4:7],  geom = "convex", 
             palette = "jco", ggtheme = theme_minimal()) + labs(subtitle = "K means clustering to visualize the 5 clusters")

# After using K mean algorithm we can see that clusters 3,4 and 5 are overlapping hence it indicates that the data is not well-suited for this type of clustering. In such cases, we can consider using density-based clustering algorithms such as DBSCAN. Compared to k-means clustering, DBSCAN is more robust to noise and can handle data with varying densities and shapes  

```


```{r}
library(dbscan)

dbscan::kNNdistplot(train_data_norm[,4:7], k =  4)
abline(h = 0.5,col="red")

#The kNN distplot with k=4 has identified a clear elbow point at 0.5, indicating a reasonable value for the epsilon parameter in the DBSCAN algorithm. This means that data points that are within a distance of 0.5 from each other are considered to be in the same cluster. The appropriate value for minPts is then chosen based on domain knowledge and experimentation.

```
```{r}

# Perform DBSCAN clustering

#Selecting numerical data to form clusters:
Training_numerical<-train_data[,c(4:7)]
#Normalizing the data:
Training_norm<-scale(Training_numerical)


db <- dbscan::dbscan(Training_norm, eps = 0.5, minPts = 25)
db

#The result of the clustering is that there are 4 clusters and 256 noise points. The clusters are labeled as 0, 1, 2, and 3, with cluster 0 containing 294 points, cluster 1 containing 3072 points, cluster 2 containing 5737 points, and clusters 3 has 30 points.

# Visualize the clustering results
library(factoextra)
fviz_cluster(db, train_data_norm[,4:7], stand = TRUE, show.clust.cent = TRUE, ellipse.type = "norm")

#Let us explore the clusters formed and try to understand how each attribute is behaving in different cluster.

#Assigning clusters to the original data:
train_set<-cbind(train_data,db$cluster)


head(train_set)

```

```{r}
#Let us explore the clusters formed and try to understand how each attribute is behaving in different cluster.

library(dplyr)
#Finding mean within each cluster to interpret the clusters:
train_set%>%group_by(db$cluster)%>%
  summarize(avg_units=mean(fuel_received_units),
            avg_cost=mean(fuel_cost_per_mmbtu),
            avg_mmbtu=mean(fuel_mmbtu_per_unit))


#From the above observation we can see that Average cost for cluster 2 is the highest and maximum Average heat is generated by cluster 1 fuel type and least heat is generated by Cluster 2 unit fuel



#Adding the cluster information to original data without dummy variable and let us use this for futher analysis.
set.seed(8627)
Part_data<-caret::createDataPartition(Fuel_data$fuel_mmbtu_per_unit,p=0.75,list = FALSE)

Data_final<-Fuel_data[train_idx,]

nrow(Data_final)

# Cluster 1 - Gas
# Cluster 2 - Oil 
# Cluster 3 - Coal

Data_final$cluster<-db$cluster

```

```{r}
library(ggplot2)

#Observing the Distribution of Fuel across Different Contracts
ggplot(Data_final, mapping = aes(factor(cluster), fill =contract_type_code))+geom_bar(position='dodge')+labs(x ='Clusters')

#Observing the Distribution of Fuel across Different Energy sources
ggplot(Data_final, mapping = aes(factor(cluster), fill =energy_source_code))+geom_bar(position='dodge')+labs(x ='Clusters')

#Observing the Distribution of Fuel across Different Fuel group codes
ggplot(Data_final, mapping = aes(factor(cluster), fill =fuel_group_code))+geom_bar(position='dodge')+labs(x ='Clusters')

#Observing the Distribution of Fuel across Different Transportation codes
ggplot(Data_final, mapping = aes(factor(cluster), fill =primary_transportation_mode_code))+geom_bar(position='dodge')+labs(x ='Clusters')
```

```{r}


### Cluster 1 -  Gas

#The analysis provides insights into the characteristics of Cluster 1, which represents the Gas fuel type. 
#The analysis suggests that Gas has the lowest average fuel cost per mmbtu compared to other fuel types, which could make it a cost-effective option for energy generation. 
#Gas has the lowest average fuel mmbtu per unit, indicating that it generates less heat content per unit compared to other fuel types. 
#Gas does not contain any ash, sulfur, and mercury content, which could make it a clean and environmentally friendly fuel option. 
#Natural gas is the energy source code for Gas, and pipelines are the most commonly used transportation type to supply this fuel type. This information could be useful in assessing the infrastructure requirements and associated costs for using Gas as a fuel type. 
#The graph indicates that Gas is mainly purchased on spot rather than through contracts, which could be an important factor to consider for procurement strategies.

### Cluster 2 -  Oil

#Oil is the most expensive type of fuel and is purchased only on spot. This suggests that the buyers are not willing to enter into long-term contracts for purchasing oil, possibly due to the high cost and uncertainty of future prices.
#Oil has a relatively low average units received in comparison to gas and coal could be due to its high cost. Customers may prefer to use other fuel types that are cheaper and more cost-effective.
#Sulfur content in oil is important as it indicates that oil may have negative environmental impacts such as air pollution



### Cluster 3 -  Coal
#Coal has the lowest average cost per unit of fuel, it also has the highest average fuel mmbtu per unit, indicating that it produces more heat energy per unit than gas or oil. This could explain why it is widely used in the US despite its environmental impact.
#Coal is purchased both on spot and on contract, but there are more spot purchases than contract purchases.
#Coal include BIT and SUB, which stand for Bituminous Coal and Sub bituminous Coal, respectively. These are two types of coal that differ in their energy content and chemical properties, and they are both widely used in the US for electricity generation and other industrial purposes.
#Presence of ash, sulfur, and mercury in coal can have significant environmental and health impacts, including air pollution, acid rain, and water pollution. It is important to note that the use of coal is declining in the US and many other countries due to these impacts, as well as the increasing availability and affordability of renewable energy sources.
```

```{r}
#Use multiple-linear regression to determine the best set of variables to predict fuel_cost_per_mmbtu. Running the multiple linear regression model to determine the best set of variables to predict fuel_cost_per_mmbtu by considering variables which were used to form clusters:

Model_data<- lm(train_data$fuel_cost_per_mmbtu~.,data=train_data)

summary(Model_data)

#Fuel received units,fuel_type_coal and fuel_type_oil best determine the fuel_cost_per_mmbtu variable.
#Checking the prediction of the above model on Test data


#We could see that the difference between predicted values and actual values is too high,which means that it is too complex and is fitting the noise in the data instead of the underlying patterns. This can result in poor performance on new, unseen data.
```




